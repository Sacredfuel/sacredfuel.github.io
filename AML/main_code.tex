\documentclass[article]{llncs}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}
\usepackage[a4paper,left=3cm,right=3cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{float}
\usepackage{amsmath,amssymb}
\usepackage{colortbl}
\usepackage{minted}
\usepackage{graphicx}


\begin{document}
%
\graphicspath{ {./images/} }
\title{Statistics 72556 Coding Assignments}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Ryan Vaz}
\institute{Hunter College}
%
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%
\maketitle              % typeset the header of the contribution
%

%
\begingroup
\let\clearpage\relax
\tableofcontents*
\endgroup
%

\section{Lecture 1 - ANN}
\subsection{Preamble}
\subsubsection{Virtual Environment}
For this class we will use a virtual environment to hold our libraries, this section will focus on installing it\\

\begin{minted}{shell}
python3 -m venv <path>
cd <path>\venv\Scripts
activate
pip install tensorflow
pip install numpy
pip install pandas
pip install pydot
pip install ipython
pip install graphviz
pip install scikit-learn
\end{minted}
\noindent To turn it off run the deactivate file in the scripts folder 
Functions to set up:
\begin{minted}{python}
import numpy as np
import pandas as pd
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
import warnings

np.random.seed(420)
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

PROJECT_ROOT_DIR = "."
CHAPTER_ID = "ann"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

warnings.filterwarnings(action="ignore", message="^internal gelsd")
\end{minted}

\subsection{Linear Regression Implementation}
\begin{minted}{python}
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
plt.plot(X, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
\end{minted}
Graph of 3x+4 = y with a little random spread
\begin{minted}{python}
X_b = np.c_[np.ones((100, 1)), X]
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
print(theta_best)
\end{minted}
Our NN's {intercept, slope}
\subsection{Batch Gradient Descent Implementation}
$\nabla f = \frac{2}{m} X^T (X\theta - y) $\\
\begin{minted}{python}
eta = 0.1 
n_iterations = 100
m = 100

theta = np.random.randn(2,1) 

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients

X_new = np.array([[0], [2]])
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_predict = X_new_b.dot(theta_best)

theta_path_bgd = []

def plot_gradient_descent(theta, eta, theta_path=None):
    m = len(X_b)
    plt.plot(X, y, "b.")
    n_iterations = 1000
    for iteration in range(n_iterations):
        if iteration < 10:   
            y_predict = X_new_b.dot(theta)
            style = "b-" if iteration > 0 else "r--"
            plt.plot(X_new, y_predict, style)
        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - eta * gradients
        if theta_path is not None:
            theta_path.append(theta)
    plt.xlabel("$x_1$", fontsize=18)
    plt.axis([0, 2, 0, 15])
    plt.title(r"$\eta = {}$".format(eta), fontsize=16)

np.random.seed(420)
theta = np.random.randn(2,1)

plt.figure(figsize=(10,4))
plt.subplot(131); plot_gradient_descent(theta, eta=0.02)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)
plt.subplot(133); plot_gradient_descent(theta, eta=0.45)

save_fig("gradient_descent_plot")
plt.show()
\end{minted}
\subsection{Linear Classifier implementation}
\begin{minted}{python}
from sklearn.metrics import f1_score
np.arange(60, 78, 0.1)
np.random.seed(1)

def plot_customers(X, y, xlabel='Inches (in)', ylabel='Pounds (lb)'): 
    colors = ['g', 'y']
    labels = ['Not Large', 'Large']
    for i, (color, label) in enumerate(zip(colors, labels)):
        plt.scatter(X[:,0][y == i], X[:,1][y == i], color=color, label=label)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)

inches = np.arange(60, 78, 0.1)
random_fluctuations = np.random.normal(scale=10, size=inches.size) 
pounds = 4 * inches - 130 + random_fluctuations

X = np.array([inches, pounds]).T
y = ((X[:,0] > 72) & (X[:,1] > 160)).astype(int)

plot_customers(X,y)
plt.legend()
plt.show()

def boundary(inches): 
    return  -3.5 * inches + 415

plt.plot(X[:,0], boundary(X[:,0]), color='k', label='Boundary') 
plot_customers(X, y)
plt.legend()
plt.show()

y_pred = []
for inches, lbs in X:
    prediction = int(lbs > -3.5 * inches + 415) # above the boudary is lighter green
    y_pred.append(prediction)

f_measure = f1_score(y_pred, y)
print(f'The f-measure is {f_measure:.2f}')

d = pd.DataFrame({'pred': y_pred, 'actual': y})
pd.crosstab(d.pred, d.actual)
\end{minted}

\subsection{Perceptron Implementation}
\begin{minted}{python}
weights = np.array([3.5, 1, -415])

def linear_classifier(X, weights):
    M = np.column_stack([X, np.ones(X.shape[0])])
    return (M @ weights > 0).astype(int)

predictions = linear_classifier(X, weights)
assert predictions.tolist() == y_pred

def predict(v, weights): 
    return int(v @ weights > 0)

def get_bias_shift(predicted, actual, learning_rate=0.1): 
    return learning_rate * (predicted - actual)

np.random.seed(0)
def train(X, y, predict=predict, adaptive_lr=1):
    M = np.column_stack([X, np.ones(X.shape[0])])
    weights = np.random.normal(size=X.shape[1] + 1)
    f_measures = []
    
    num = N
    for k in range(1, num):
        y_pred = linear_classifier(X, weights)
        f_measures.append(f1_score(y_pred, y))
        
        for i, actual in enumerate(y):
            predicted = predict(M[i], weights)
            bias_shift = get_bias_shift(predicted, actual)
            delta = bias_shift * M[i]
            if adaptive_lr:
                 delta /= k
            weights -=  delta 
    return weights, f_measures

N = 1000
weights, f_measures = train(X, y, adaptive_lr=0)

print(f'The f-measure after {N} iterations is {f_measures[-1]:.2f}')

plt.plot(range(len(f_measures)), f_measures)
plt.xlabel('Iteration')
plt.ylabel('F-measure')
plt.show()

weights, f_measures = train(X, y, adaptive_lr=1)
print(f'The f-measure after 1000 iterations is {f_measures[-1]:.2f}') 

plt.plot(range(len(f_measures)), f_measures)
plt.xlabel('Iteration')
plt.ylabel('F-measure')
plt.show()

inches_coef, lbs_coef, bias = weights

def new_boundary(inches):
    return -(inches_coef * inches + bias) / lbs_coef

fig, ax = plt.subplots(figsize=(7, 7))

plt.plot(X[:,0], new_boundary(X[:,0]), color='k', linestyle='--',
         label='Trained Boundary', linewidth=2)
plt.plot(X[:,0], boundary(X[:,0]), color='k', label='Optimal Boundary', linewidth=2)

plot_customers(X, y)
plt.legend()
plt.show()
\end{minted}

\section{Lecture 2 - MLP Architecture}

%TODO: needs to be updated


\end{document}
